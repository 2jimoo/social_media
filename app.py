# Streamlit >= 1.25 ê¶Œì¥ (st.chat_input / st.chat_message ì‚¬ìš©)
# pip install streamlit requests beautifulsoup4 langchain langchain-community sentence-transformers faiss-cpu

import json
import time
from typing import Dict, Iterator, List, Optional

import requests
import streamlit as st

# ----- LangChain / RAG ê´€ë ¨ -----
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

st.set_page_config(page_title="Streamlit Chat + Sidebar (Ollama + RAG)", layout="centered")

# ---------------------- ì„¤ì • / ì„¸ì…˜ ìƒíƒœ ----------------------
if "messages" not in st.session_state:
    st.session_state.messages: List[Dict[str, str]] = []
if "mode" not in st.session_state:
    st.session_state.mode = "Facebook"
if "system_prompts" not in st.session_state:
    st.session_state.system_prompts = {
        "Facebook": "You are a Social Media Policy Consultant specializing in Facebook. Answer in Korean. Be concise, cite relevant Facebook policies when helpful (paraphrase).",
        "Instagram": "You are a Social Media Policy Consultant specializing in Instagram. Answer in Korean. Be concise, refer to Instagram and Meta policies when helpful (paraphrase).",
        "Twitter": "You are a Social Media Policy Consultant specializing in X (Twitter). Answer in Korean. Be concise, refer to X rules when helpful (paraphrase).",
    }
# ê° ëª¨ë“œë³„ URL / ë²¡í„°ìŠ¤í† ì–´ / ì„ë² ë”© ìºì‹œ
if "policy_urls" not in st.session_state:
    st.session_state.policy_urls = {
        # í•„ìš”ì‹œ ì›í•˜ì‹œëŠ” ê°€ì´ë“œ URLë¡œ ë°”ê¾¸ì„¸ìš”
        "Facebook": "https://transparency.fb.com/policies/ad-standards/",
        "Instagram": "https://transparency.fb.com/policies/ad-standards/",
        "Twitter": "https://business.x.com/en/help/policy-center/ads-policies.html",
    }
if "vstores" not in st.session_state:
    st.session_state.vstores: Dict[str, Optional[FAISS]] = {"Facebook": None, "Instagram": None, "Twitter": None}
if "embeddings" not in st.session_state:
    # í•œ ë²ˆë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ìºì‹œ)
    st.session_state.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ---------------------- í•¨ìˆ˜: ì •ì±… í˜ì´ì§€ ë¡œë“œ/íŒŒì‹±/ìƒ‰ì¸ ----------------------
def fetch_policy_text(url: str, timeout: int = 30) -> str:
    """ì •ì±… URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ ì •ì œ."""
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    # ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = soup.get_text(separator="\n")
    # ê°„ë‹¨ ì •ì œ
    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]  # ë¹ˆ ì¤„ ì œê±°
    return "\n".join(lines)

def build_vectorstore_from_text(text: str, source_url: str) -> FAISS:
    """í…ìŠ¤íŠ¸ë¥¼ chunkë¡œ ë¶„í• í•˜ê³  FAISS ë²¡í„°ìŠ¤í† ì–´(ë©”ëª¨ë¦¬ ìƒì£¼)ë¥¼ ìƒì„±."""
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)
    chunks = splitter.split_text(text)

    docs = [
        Document(page_content=chunk, metadata={"source": source_url, "chunk_index": i})
        for i, chunk in enumerate(chunks)
    ]
    vs = FAISS.from_documents(docs, st.session_state.embeddings)
    return vs

def ensure_vectorstore(mode: str, force_reload: bool = False) -> Optional[str]:
    """
    í•´ë‹¹ ëª¨ë“œì˜ ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ ìˆìœ¼ë©´ URLì„ íŒŒì‹±í•´ì„œ ìƒì„±.
    force_reload=Trueë©´ í•­ìƒ ë‹¤ì‹œ êµ¬ì„±.
    ì—ëŸ¬ ë©”ì‹œì§€(ìˆë‹¤ë©´) ë°˜í™˜.
    """
    try:
        if force_reload or st.session_state.vstores.get(mode) is None:
            url = st.session_state.policy_urls.get(mode)
            if not url:
                return f"[ì˜¤ë¥˜] {mode} ëª¨ë“œì˜ ì •ì±… URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
            with st.spinner(f"{mode} ê´‘ê³  ê°€ì´ë“œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
                text = fetch_policy_text(url)
                vs = build_vectorstore_from_text(text, url)
                st.session_state.vstores[mode] = vs
                st.toast(f"{mode} ì •ì±… ë¬¸ì„œë¥¼ {len(vs.index_to_docstore_id)}ê°œ ì²­í¬ë¡œ ìƒ‰ì¸ ì™„ë£Œ")
        return None
    except requests.RequestException as e:
        return f"[ì˜¤ë¥˜] ì •ì±… í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}"
    except Exception as e:
        return f"[ì˜¤ë¥˜] ìƒ‰ì¸ ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}"

def retrieve_context(query: str, mode: str, k: int = 5) -> str:
    """ìœ ì € ì¿¼ë¦¬ì— ë§ê²Œ ìƒìœ„ kê°œ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨."""
    vs = st.session_state.vstores.get(mode)
    if not vs:
        return ""
    try:
        docs = vs.similarity_search(query, k=k)
    except Exception:
        # ì¼ë¶€ ë²„ì „/í™˜ê²½ì—ì„œ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ëŒ€ë¹„
        docs = []
    if not docs:
        return ""
    context_blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "unknown")
        idx = d.metadata.get("chunk_index", "?")
        context_blocks.append(f"[ì½˜í…ìŠ¤íŠ¸ {i} | {src} | chunk {idx}]\n{d.page_content}")
    return "\n\n".join(context_blocks)

# ---------------------- Ollama Chat ----------------------
def _build_system_prompt(mode: str) -> str:
    return st.session_state.system_prompts.get(mode, st.session_state.system_prompts["Facebook"]) + \
        "\n- Use bullet points when listing.\n- Ask for missing context only if essential.\n- Keep answers practical and actionable.\n- If provided, use the policy context below first."


def _to_ollama_messages(user_assistant_msgs, mode, rag_context=""):
    sys_prompt = _build_system_prompt(mode)
    if rag_context:
        sys_prompt += f"\n\n### Policy Context (RAG)\n{rag_context}\n\n" \
                      "ì§€ì¹¨: ìœ„ 'Policy Context'ë¥¼ ìš°ì„  ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”. ì¶œì²˜(URL)ì™€ ê·¼ê±° ë¬¸êµ¬ë¥¼ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ìš”ì•½/ì¸ìš©(ì˜ì—­)í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”."

    msgs = []
    # âœ… system â†’ userë¡œ ë³€í™˜
    msgs.append({"role": "user", "content": f"[ì‹œìŠ¤í…œ ì§€ì¹¨]\n{sys_prompt}"})

    for m in user_assistant_msgs:
        msgs.append(m)

    return msgs

def ollama_chat_stream(
    base_url: str,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.3,
    top_p: float = 0.9,
    max_tokens: int = 1024,
) -> Iterator[str]:
    """/api/chat ìŠ¤íŠ¸ë¦¬ë° ì œë„ˆë ˆì´í„°. Streamlitì˜ st.write_streamê³¼ í•¨ê»˜ ì‚¬ìš©."""
    url = base_url.rstrip("/") + "/api/chat"
    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": float(temperature),
            "top_p": float(top_p),
            "num_predict": int(max_tokens),
        },
    }
    try:
        with requests.post(url, json=payload, stream=True, timeout=(5, 600)) as r:
            r.raise_for_status()
            for line in r.iter_lines(decode_unicode=True):
                if not line:
                    continue
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                msg = data.get("message", {})
                if "content" in msg:
                    yield msg["content"]
                if data.get("done"):
                    break
    except requests.RequestException as e:
        yield f"\n[ì˜¤ë¥˜] Ollama ì„œë²„ í†µì‹  ì‹¤íŒ¨: {e}"

def ollama_chat_once(
    base_url: str,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.3,
    top_p: float = 0.9,
    max_tokens: int = 1024,
) -> str:
    """ìŠ¤íŠ¸ë¦¬ë°ì´ ë¶ˆê°€í•œ í™˜ê²½ì„ ìœ„í•œ ë‹¨ë°œì„± í˜¸ì¶œ."""
    url = base_url.rstrip("/") + "/api/chat"
    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": float(temperature),
            "top_p": float(top_p),
            "num_predict": int(max_tokens),
        },
    }
    try:
        r = requests.post(url, json=payload, timeout=(5, 120))
        r.raise_for_status()
        data = r.json()
        return data.get("message", {}).get("content", "")
    except requests.RequestException as e:
        return f"[ì˜¤ë¥˜] Ollama ì„œë²„ í†µì‹  ì‹¤íŒ¨: {e}"

# ---------------------- ì¤‘ì•™ ì˜ì—­: UI ----------------------
st.title("ğŸ•Š Social Media Policy Consultant (with RAG)")

with st.sidebar:
    st.header("ë©”ë‰´")
    st.caption("ìƒë‹´í•˜ê³  ì‹¶ì€ SNS ì¢…ë¥˜ë¥¼ ë°”ê¿” ë³´ì„¸ìš”. (ë²„íŠ¼ í´ë¦­ ì‹œ í•´ë‹¹ ê°€ì´ë“œë¥¼ í¬ë¡¤ë§/ìƒ‰ì¸)")
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Facebook", use_container_width=True):
            st.session_state.mode = "Facebook"
            err = ensure_vectorstore("Facebook")  # ìƒ‰ì¸ ë³´ì¥
            if err:
                st.error(err)
            else:
                st.toast("Facebookìœ¼ë¡œ ì „í™˜ ë° ì •ì±… ìƒ‰ì¸ ì™„ë£Œ")
    with col2:
        if st.button("Instagram", use_container_width=True):
            st.session_state.mode = "Instagram"
            err = ensure_vectorstore("Instagram")
            if err:
                st.error(err)
            else:
                st.toast("Instagramìœ¼ë¡œ ì „í™˜ ë° ì •ì±… ìƒ‰ì¸ ì™„ë£Œ")
    with col3:
        if st.button("Twitter", use_container_width=True):
            st.session_state.mode = "Twitter"
            err = ensure_vectorstore("Twitter")
            if err:
                st.error(err)
            else:
                st.toast("Twitterë¡œ ì „í™˜ ë° ì •ì±… ìƒ‰ì¸ ì™„ë£Œ")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", type="secondary", use_container_width=True):
        st.session_state.messages = []
        st.success("ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

    st.subheader("RAG ì„¤ì •")
    st.caption("ì •ì±… URLì„ ììœ ë¡­ê²Œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    for m in ["Facebook", "Instagram", "Twitter"]:
        st.session_state.policy_urls[m] = st.text_input(
            f"{m} ê´‘ê³  ê°€ì´ë“œ URL",
            value=st.session_state.policy_urls[m],
            help=f"{m} ê³µì‹ ê´‘ê³  ê°€ì´ë“œ(ì •ì±…) í˜ì´ì§€ URL"
        )

    if st.button("í˜„ì¬ ëª¨ë“œ ì •ì±… ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°(ì¬ìƒ‰ì¸)", use_container_width=True):
        msg = ensure_vectorstore(st.session_state.mode, force_reload=True)
        if msg:
            st.error(msg)
        else:
            st.success(f"{st.session_state.mode} ì •ì±…ì„ ì¬ìƒ‰ì¸í–ˆìŠµë‹ˆë‹¤.")

    st.subheader("Ollama ì„¤ì •")
    ollama_url = st.text_input("Ollama Base URL", value="http://localhost:11434", help="ì˜ˆ: http://localhost:11434")
    model_name = st.text_input("ëª¨ë¸ ì´ë¦„", value="Phi3", help="ollama run ìœ¼ë¡œ ë°›ì€ ëª¨ë¸ëª…")
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.5, value=0.3, step=0.1)
    top_p = st.slider("top_p", min_value=0.0, max_value=1.0, value=0.9, step=0.05)
    max_tokens = st.number_input("max_tokens (ì‘ë‹µ ìµœëŒ€ í† í°)", min_value=1, max_value=8192, value=1024, step=64)

st.caption(f"í˜„ì¬ ì„ íƒëœ SNS: **{st.session_state.mode}**  |  ëª¨ë¸: **{model_name}**  |  ì„œë²„: {ollama_url}")

# ìµœì´ˆ ì§„ì… ì‹œ í˜„ì¬ ëª¨ë“œ ìƒ‰ì¸ ë³´ì¥
_ = ensure_vectorstore(st.session_state.mode)

# ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì…ë ¥ì°½
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”â€¦")

if user_input:
    # ìœ ì € ë©”ì‹œì§€ ì¶”ê°€/í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # --- RAG: í˜„ì¬ ëª¨ë“œì˜ ì •ì±…ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ---
    rag_context = retrieve_context(user_input, st.session_state.mode, k=3)
    print(f"rag_context: {rag_context}")

    # Ollamaë¡œ ë³´ë‚¼ ëŒ€í™” ë©”ì‹œì§€ êµ¬ì„± (ì»¨í…ìŠ¤íŠ¸ë¥¼ systemì— ì£¼ì…)
    ollama_msgs = _to_ollama_messages(st.session_state.messages, st.session_state.mode, rag_context=rag_context)
    print(f"ollama_msgs: {ollama_msgs}")

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì˜ì—­ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
    with st.chat_message("assistant"):
        try:
            chunks = ollama_chat_stream(
                base_url=ollama_url,
                model=model_name,
                messages=ollama_msgs,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )
            full_text = st.write_stream(chunks)
        except Exception:
            full_text = ollama_chat_once(
                base_url=ollama_url,
                model=model_name,
                messages=ollama_msgs,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )
            st.markdown(full_text)

    # ì„¸ì…˜ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_text})

# ---------------------- í•˜ë‹¨ ë„ì›€ë§ ----------------------
with st.expander("â„¹ï¸ ì‚¬ìš© íŒ"):
    st.markdown(
        """
        - ì‚¬ì´ë“œë°” SNS ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ í•´ë‹¹ ê´‘ê³  ê°€ì´ë“œ(URL)ë¥¼ í¬ë¡¤ë§/ë¶„í• /ë²¡í„°í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ë‚´ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.
        - ì´í›„ ìœ ì € ì§ˆë¬¸ ì‹œ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ ê·¼ê±°ë¥¼ ì°¾ì•„ **Policy Context (RAG)** ë¡œ ì£¼ì…í•˜ë¯€ë¡œ ë³´ë‹¤ ì •í™•í•œ ì •ì±… ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        - URLì€ ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥í•˜ë©°, â€œì¬ìƒ‰ì¸â€ ë²„íŠ¼ìœ¼ë¡œ ì¦‰ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤.
        - ì„ë² ë”© ëª¨ë¸ì€ `sentence-transformers/all-MiniLM-L6-v2`, ë²¡í„°ìŠ¤í† ì–´ëŠ” ë©”ëª¨ë¦¬ ìƒì£¼ FAISSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - Ollama ì„œë²„ê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: `ollama serve` í›„ `ollama run {ëª¨ë¸}`
        - ì‘ë‹µì´ ì—†ìœ¼ë©´ ëª¨ë¸ëª…, ì„œë²„ URL, ë°©í™”ë²½/í”„ë¡ì‹œ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.
        - ì¥ë¬¸ ìš”ì•½/ë¶„ì„ ë“±ì€ `max_tokens`ë¥¼ ëŠ˜ë ¤ ë³´ì„¸ìš”.
        """
    )
